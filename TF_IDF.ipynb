{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-IDF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMT5Skwpoctb",
        "outputId": "e30edeeb-01af-43c6-ddaf-077d9636445f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import re\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading in and writing files\n",
        "def load_data(file):\n",
        "  with open(file, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "  return data\n",
        "\n",
        "def write_data(file, data):\n",
        "  with open(file, \"w\", encoding = \"utf-8\") as f:\n",
        "    json.dump(data, f, indent = 4)"
      ],
      "metadata": {
        "id": "TTthl9qJppYs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to remove from descriptions:\n",
        "\n",
        "1. stopwords\n",
        "2. patterns like (AC/2000/142)\n",
        "3. dates "
      ],
      "metadata": {
        "id": "DGPibG7vsNcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stops(text, stop_words):\n",
        "  text = re.sub(r\"AC\\/\\d{1,4}\\/\\d{1,4}\", \"\", text)                               # removing patterns from desription such as : (AC/2000/147)\n",
        "  words = text.split()\n",
        "  final = [word for word in words if word not in stop_words]                     # Removing stopwords\n",
        "  final = \" \".join(final)\n",
        "  final = final.translate(str.maketrans(\"\",\"\",string.punctuation))               # Removing Punctuations from the text\n",
        "  final = \"\".join([i for i in final if not i.isdigit()])                       # Removing digits\n",
        "  while \"  \" in final:\n",
        "    final = final.replace(\"  \", \" \")\n",
        "  return final\n",
        "\n",
        "def clean_docs(docs):\n",
        "  stop_words = stopwords.words('english')\n",
        "  \n",
        "  # Removing months names\n",
        "  months = load_data('months.json')\n",
        "  #print(months)\n",
        "  stops = stop_words + months\n",
        "\n",
        "  doc = [remove_stops(doc, stops) for doc in docs]\n",
        "  return doc"
      ],
      "metadata": {
        "id": "ljAj6-T9sk_e"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading in data : data has description about violence and victim names\n",
        "descriptions = load_data('trc_dn.json')['descriptions']\n",
        "names = load_data('trc_dn.json')['names']"
      ],
      "metadata": {
        "id": "Is2RDhj1rDD8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_docs = clean_docs(descriptions)\n",
        "#print(cleaned_docs)\n",
        "print(len(cleaned_docs))"
      ],
      "metadata": {
        "id": "Tr-yYhasraFn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1541e48-1909-489a-d911-67d11980fb73"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(lowercase=False,\n",
        "                             max_features = 100,\n",
        "                             min_df = 3,\n",
        "                             ngram_range = (1,3),\n",
        "                             stop_words = 'english')\n",
        "\n",
        "vectors = vectorizer.fit_transform(cleaned_docs)"
      ],
      "metadata": {
        "id": "0JyJeueUSIxf"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = vectorizer.get_feature_names()\n",
        "#print(len(feature_names))\n",
        "\n",
        "# Doc-word dense matrix\n",
        "dense = vectors.todense()\n",
        "#print(dense.shape)\n",
        "\n",
        "denselist = dense.tolist()\n",
        "# print(len(denselist))"
      ],
      "metadata": {
        "id": "IvG_0mcNUsQl"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we are taking values which are greater that 0 (meaning corresponding words) from TFIDF matrix for each document\n",
        "\n",
        "all_keywords = []\n",
        "for row in denselist:\n",
        "  #print(description)\n",
        "  x = 0\n",
        "  keywords = []\n",
        "  for value in row:\n",
        "    if value > 0.0:\n",
        "      keywords.append(feature_names[x])\n",
        "    x = x+1\n",
        "  all_keywords.append(keywords)\n",
        "\n",
        "# here we are looking at a complete description and important keywords for that description that we have calculated using TFIDF matrix.\n",
        "print(descriptions[0])\n",
        "print(all_keywords[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81n0D5L1Yo3t",
        "outputId": "2b28bff0-22ab-4d65-facf-35274c5c9e4a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An ANCYL member who was shot and severely injured by SAP members at Lephoi, Bethulie, Orange Free State (OFS) on 17 April 1991. Police opened fire on a gathering at an ANC supporter's house following a dispute between two neighbours, one of whom was linked to the ANC and the other to the SAP and a councillor.\n",
            "['ANC', 'ANC supporters', 'An', 'Police', 'SAP', 'house', 'injured', 'member', 'members', 'severely', 'shot', 'supporters']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now er eant to do the clustering:\n",
        "# We want to take all those keywords fom TFIDF and cluster them using Kmeans\n",
        "model = KMeans(n_clusters=20, init=\"k-means++\", max_iter=100, n_init=1)\n",
        "model.fit(vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnER3XeQaWtt",
        "outputId": "bc828128-33d6-42fc-c2d9-304991f36cbc"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(max_iter=100, n_clusters=20, n_init=1)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = vectorizer.get_feature_names()\n",
        "\n",
        "# Saving  the cluters in a file\n",
        "with open (\"trc_cluster_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for i in range(20):\n",
        "        f.write(f\"Cluster {i}\")\n",
        "        f.write(\"\\n\")\n",
        "        for ind in order_centroids[i, :10]:\n",
        "            f.write (' %s' % terms[ind],)\n",
        "            f.write(\"\\n\")\n",
        "        f.write(\"\\n\")\n",
        "        f.write(\"\\n\")"
      ],
      "metadata": {
        "id": "0j5FE8Nte_eb"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference : https://www.youtube.com/watch?v=i74DVqMsRWY&list=PL2VXyKi-KpYttggRATQVmgFcQst3z6OlX&index=7\n",
        "\n",
        "https://github.com/wjbmattingly/topic_modeling_textbook/blob/main/lessons/02_tf_idf_official.py"
      ],
      "metadata": {
        "id": "brwXHvUwfwJW"
      }
    }
  ]
}